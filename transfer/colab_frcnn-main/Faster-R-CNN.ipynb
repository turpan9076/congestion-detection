{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1765968800275,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "RPG6hj8j7LmC"
   },
   "outputs": [],
   "source": [
    "\n",
    "#pathの指定(colab_frcnn-main直下まで)\n",
    "bdd_xml=\"./mydatasets/xml\"\n",
    "bdd_img=\"./mydatasets/img\"\n",
    "test_path=\"../../input_with_line/20250918/C00452\"\n",
    "\n",
    "#datasetのクラス指定\n",
    "dataset_class=['Car', 'Track', 'Bus', 'Ambulance', 'Motorcycle']\n",
    "#表示したいラベルの色の指定\n",
    "#注意！！一番最初は背景クラスを示すので(0,0,0)にする(それ以外は自由)\n",
    "colors = ((0,0,0),(255,0,0),(0,255,0),(0,0,255),(100,100,100),(50,50,50))\n",
    "\n",
    "#ハイパーパラメータの指定\n",
    "epochs=10\n",
    "batch_size=1\n",
    "scale=416#画像のスケール設定(縦の大きさを入力)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1765968802173,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "_KHZ53cc7-XV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "\n",
    "\n",
    "class xml2list(object):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "\n",
    "    def __call__(self, xml_path):\n",
    "\n",
    "        ret = []\n",
    "        xml = ET.parse(xml_path).getroot()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        zz=0\n",
    "\n",
    "        for zz,obj in enumerate(xml.iter('object')):\n",
    "\n",
    "            label = obj.find('name').text\n",
    "\n",
    "            ##指定クラスのみ\n",
    "\n",
    "            if label in self.classes :\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(self.classes.index(label))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        num_objs = zz +1\n",
    "\n",
    "        anno = {'bboxes':boxes, 'labels':labels}\n",
    "\n",
    "        return anno,num_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1765969964613,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "NfoS2M8w8Xph"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self,image_dir,xml_paths,scale,classes):\n",
    "\n",
    "            super().__init__()\n",
    "            self.image_dir = image_dir\n",
    "            self.xml_paths = xml_paths\n",
    "            self.image_ids = sorted(glob('{}/*'.format(xml_paths)))\n",
    "            self.scale=scale\n",
    "            self.classes=classes\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                                            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "            # 入力画像の読み込み\n",
    "            #image_id=self.image_ids[index].split(\"/\")[-1].split(\".\")[0]\n",
    "            #image = Image.open(f\"{self.image_dir}/{image_id}.jpg\")\n",
    "            filename = os.path.basename(self.image_ids[index])\n",
    "            image_id = os.path.splitext(filename)[0]\n",
    "\n",
    "            image = Image.open(os.path.join(self.image_dir, image_id + \".jpg\"))\n",
    "\n",
    "            #画像のスケール変換\n",
    "            t_scale_tate=self.scale ##目標のスケール(縦)\n",
    "            #縮小比を計算\n",
    "            ratio=t_scale_tate/image.size[1]\n",
    "            ##目標横スケールを計算\n",
    "            t_scale_yoko=image.size[0]*ratio\n",
    "            t_scale_yoko=int(t_scale_yoko)\n",
    "\n",
    "            #print('縮小前:',image.size)\n",
    "            #print('縮小率:',ratio)\n",
    "            #リサイズ\n",
    "            image = image.resize((t_scale_yoko,t_scale_tate))\n",
    "            #print('縮小後:',image.size)\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            transform_anno = xml2list(self.classes)\n",
    "            path_xml=f'{self.xml_paths}/{image_id}.xml'\n",
    "\n",
    "\n",
    "            annotations, _ = transform_anno(path_xml) # obje_num is not needed here\n",
    "\n",
    "            boxes_list = annotations['bboxes']\n",
    "            labels_list = annotations['labels']\n",
    "\n",
    "            # Handle cases where no bounding boxes are found for the specified classes\n",
    "            if not boxes_list:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                area = torch.zeros((0,), dtype=torch.float32)\n",
    "                iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.as_tensor(boxes_list, dtype=torch.int64)\n",
    "                labels = torch.as_tensor(labels_list, dtype=torch.int64)\n",
    "\n",
    "                #bboxの縮小\n",
    "                #print('縮小前:',boxes)\n",
    "                boxes = boxes * ratio\n",
    "                #print('縮小後:',boxes)\n",
    "\n",
    "                area = (boxes[:, 3]-boxes[:, 1]) * (boxes[:, 2]-boxes[:, 0])\n",
    "                area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "                # iscrowd should match the number of actual objects found\n",
    "                iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels+1\n",
    "            target[\"image_id\"] = torch.tensor([index])\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "            return image, target,image_id\n",
    "\n",
    "        def __len__(self):\n",
    "\n",
    "            return len(self.image_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1765969970393,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "upnmtKtk8d_B"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataloader (data,dataset_class,batch_size,scale=720):\n",
    "    xml_paths=data[0]\n",
    "    image_dir1=data[1]\n",
    "    dataset = MyDataset(image_dir1,xml_paths,scale,dataset_class)\n",
    "\n",
    "    #データのロード\n",
    "    torch.manual_seed(2020)\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1765969971684,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "hJ1RhiKe8hwJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def model ():\n",
    "    #モデルの定義\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes=len(dataset_class)+1\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2358931,
     "status": "ok",
     "timestamp": 1765972332085,
     "user": {
      "displayName": "たはけん",
      "userId": "00195304948135574549"
     },
     "user_tz": -540
    },
    "id": "NNvL4N538kaB",
    "outputId": "4725e0cd-64e7-4774-b313-086f9e78e1b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10x\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10x\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #1 Iteration #10 loss: 0.1657692939043045\n",
      "epoch #1 Iteration #20 loss: 0.30838632583618164\n",
      "epoch #1 Iteration #30 loss: 0.31695228815078735\n",
      "epoch #1 Iteration #40 loss: 0.3969959616661072\n",
      "epoch #1 Iteration #50 loss: 0.1945684254169464\n",
      "epoch #1 Iteration #60 loss: 0.18708427250385284\n",
      "epoch #1 Iteration #70 loss: 0.2812070846557617\n",
      "epoch #1 Iteration #80 loss: 0.19812512397766113\n",
      "epoch #1 Iteration #90 loss: 0.3675290644168854\n",
      "epoch #2 Iteration #10 loss: 0.29136836528778076\n",
      "epoch #2 Iteration #20 loss: 0.25356727838516235\n",
      "epoch #2 Iteration #30 loss: 0.22632044553756714\n",
      "epoch #2 Iteration #40 loss: 0.5186583399772644\n",
      "epoch #2 Iteration #50 loss: 0.20700426399707794\n",
      "epoch #2 Iteration #60 loss: 0.13615567982196808\n",
      "epoch #2 Iteration #70 loss: 0.1884901076555252\n",
      "epoch #2 Iteration #80 loss: 0.21921901404857635\n",
      "epoch #2 Iteration #90 loss: 0.308813214302063\n",
      "epoch #3 Iteration #10 loss: 0.17595195770263672\n",
      "epoch #3 Iteration #20 loss: 0.1432512253522873\n",
      "epoch #3 Iteration #30 loss: 0.13833658397197723\n",
      "epoch #3 Iteration #40 loss: 0.10704075545072556\n",
      "epoch #3 Iteration #50 loss: 0.11750369518995285\n",
      "epoch #3 Iteration #60 loss: 0.26121050119400024\n",
      "epoch #3 Iteration #70 loss: 0.15321439504623413\n",
      "epoch #3 Iteration #80 loss: 0.10598733276128769\n",
      "epoch #3 Iteration #90 loss: 0.1545189470052719\n",
      "epoch #4 Iteration #10 loss: 0.13877899944782257\n",
      "epoch #4 Iteration #20 loss: 0.12428808957338333\n",
      "epoch #4 Iteration #30 loss: 0.17955785989761353\n",
      "epoch #4 Iteration #40 loss: 0.09089295566082001\n",
      "epoch #4 Iteration #50 loss: 0.14195100963115692\n",
      "epoch #4 Iteration #60 loss: 0.1644652634859085\n",
      "epoch #4 Iteration #70 loss: 0.14737464487552643\n",
      "epoch #4 Iteration #80 loss: 0.10874934494495392\n",
      "epoch #4 Iteration #90 loss: 0.07984643429517746\n",
      "epoch #5 Iteration #10 loss: 0.10401518642902374\n",
      "epoch #5 Iteration #20 loss: 0.07708019018173218\n",
      "epoch #5 Iteration #30 loss: 0.07501806318759918\n",
      "epoch #5 Iteration #40 loss: 0.11553436517715454\n",
      "epoch #5 Iteration #50 loss: 0.07672177255153656\n",
      "epoch #5 Iteration #60 loss: 0.06403495371341705\n",
      "epoch #5 Iteration #70 loss: 0.0728134959936142\n",
      "epoch #5 Iteration #80 loss: 0.13600118458271027\n",
      "epoch #5 Iteration #90 loss: 0.24385125935077667\n",
      "epoch #6 Iteration #10 loss: 0.0660579651594162\n",
      "epoch #6 Iteration #20 loss: 0.11648186296224594\n",
      "epoch #6 Iteration #30 loss: 0.07217318564653397\n",
      "epoch #6 Iteration #40 loss: 0.10049231350421906\n",
      "epoch #6 Iteration #50 loss: 0.0881667286157608\n",
      "epoch #6 Iteration #60 loss: 0.05524977296590805\n",
      "epoch #6 Iteration #70 loss: 0.09886980056762695\n",
      "epoch #6 Iteration #80 loss: 0.11604882776737213\n",
      "epoch #6 Iteration #90 loss: 0.09557930380105972\n",
      "epoch #7 Iteration #10 loss: 0.35717877745628357\n",
      "epoch #7 Iteration #20 loss: 0.11285306513309479\n",
      "epoch #7 Iteration #30 loss: 0.12009664624929428\n",
      "epoch #7 Iteration #40 loss: 0.10357753187417984\n",
      "epoch #7 Iteration #50 loss: 0.09180624037981033\n",
      "epoch #7 Iteration #60 loss: 0.09247568994760513\n",
      "epoch #7 Iteration #70 loss: 0.08579980581998825\n",
      "epoch #7 Iteration #80 loss: 0.06773097813129425\n",
      "epoch #7 Iteration #90 loss: 0.09850146621465683\n",
      "epoch #8 Iteration #10 loss: 0.06065484508872032\n",
      "epoch #8 Iteration #20 loss: 0.09358351677656174\n",
      "epoch #8 Iteration #30 loss: 0.06432487815618515\n",
      "epoch #8 Iteration #40 loss: 0.07750728726387024\n",
      "epoch #8 Iteration #50 loss: 0.06825771182775497\n",
      "epoch #8 Iteration #60 loss: 0.05439579486846924\n",
      "epoch #8 Iteration #70 loss: 0.044686637818813324\n",
      "epoch #8 Iteration #80 loss: 0.04459740221500397\n",
      "epoch #8 Iteration #90 loss: 0.04488350823521614\n",
      "epoch #9 Iteration #10 loss: 0.0413920134305954\n",
      "epoch #9 Iteration #20 loss: 0.0683949813246727\n",
      "epoch #9 Iteration #30 loss: 0.08052843064069748\n",
      "epoch #9 Iteration #40 loss: 0.06495222449302673\n",
      "epoch #9 Iteration #50 loss: 0.04774732515215874\n",
      "epoch #9 Iteration #60 loss: 0.04174843430519104\n",
      "epoch #9 Iteration #70 loss: 0.06379524618387222\n",
      "epoch #9 Iteration #80 loss: 0.05646169185638428\n",
      "epoch #9 Iteration #90 loss: 0.08321381360292435\n",
      "epoch #10 Iteration #10 loss: 0.04639308899641037\n",
      "epoch #10 Iteration #20 loss: 0.05128461495041847\n",
      "epoch #10 Iteration #30 loss: 0.039463385939598083\n",
      "epoch #10 Iteration #40 loss: 0.048947058618068695\n",
      "epoch #10 Iteration #50 loss: 0.029151784256100655\n",
      "epoch #10 Iteration #60 loss: 0.10124281048774719\n",
      "epoch #10 Iteration #70 loss: 0.04968751221895218\n",
      "epoch #10 Iteration #80 loss: 0.06253817677497864\n",
      "epoch #10 Iteration #90 loss: 0.0264320969581604\n"
     ]
    }
   ],
   "source": [
    "data_ALL=[bdd_xml,bdd_img]\n",
    "train_dataloader=dataloader(data_ALL,dataset_class,batch_size,scale)\n",
    "\n",
    "model=model()\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "num_epochs = epochs\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()#学習モードに移行\n",
    "\n",
    "loss_list=[]\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epo=[]\n",
    "\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "\n",
    "        images, targets, image_ids = batch#####　batchはそのミニバッジのimage、tagets,image_idsが入ってる\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "        ##学習モードでは画像とターゲット（ground-truth）を入力する\n",
    "        ##返り値はdict[tensor]でlossが入ってる。（RPNとRCNN両方のloss）\n",
    "        loss_dict= model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #lossの保存\n",
    "        loss_epo.append(loss_value)\n",
    "\n",
    "        if (i+1) % 10== 0:\n",
    "          print(f\"epoch #{epoch+1} Iteration #{i+1} loss: {loss_value}\")\n",
    "\n",
    "\n",
    "    #Epochごとのlossの保存\n",
    "    loss_list.append(np.mean(loss_epo))\n",
    "    torch.save(model, './model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_class=dataset_class\n",
    "data_class.insert(0, \"__background__\")\n",
    "classes = tuple(data_class)\n",
    "\n",
    "#学習済みモデルで推論する場合\n",
    "import torch\n",
    "\n",
    "model = torch.load(\n",
    "    \"./model_mydatasets.pth\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=False\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 保存先フォルダ\n",
    "save_dir = \"output_mydatasets\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for imgfile in sorted(glob.glob(test_path + '/*')):\n",
    "\n",
    "    img = cv2.imread(imgfile)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])\n",
    "\n",
    "    for i, box in enumerate(prediction[0]['boxes']):\n",
    "        score = prediction[0]['scores'][i].cpu().numpy()\n",
    "        if score > 0.5:\n",
    "            score = round(float(score), 2)\n",
    "            cat = prediction[0]['labels'][i].cpu().numpy()\n",
    "            txt = '{} {}'.format(classes[int(cat)], str(score))\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "            c = colors[int(cat)]\n",
    "            box = box.cpu().numpy().astype('int')\n",
    "\n",
    "            cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), c, 2)\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                (box[0], box[1] - cat_size[1] - 2),\n",
    "                (box[0] + cat_size[0], box[1] - 2),\n",
    "                c,\n",
    "                -1\n",
    "            )\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                txt,\n",
    "                (box[0], box[1] - 2),\n",
    "                font,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "    # RGB → BGR に戻して保存\n",
    "    save_path = os.path.join(save_dir, os.path.basename(imgfile))\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(save_path, img_bgr)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOIVBrHTY8m/HNWiV4hkFFn",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
